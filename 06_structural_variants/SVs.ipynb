{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#author: Mathieu HÃ©nault\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from Bio import SeqIO\n",
    "from Bio import SeqRecord\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import mannwhitneyu\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='ticks', font='DejaVu Sans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import assemblies \n",
    "\n",
    "FASTA={}\n",
    "fasta={}\n",
    "\n",
    "fasta_temp=[]\n",
    "with open('/Users/mathieu/mhenault_landrylab/Sequences/ref_genomes/S288C_pacbio/S288c.genome.fa') as file_in:\n",
    "    for j in SeqIO.parse(file_in, \"fasta\"):\n",
    "        fasta_temp.append(j)\n",
    "# whole-genome concatenated sequence\n",
    "FASTA=\"\".join([str(j.seq) for j in fasta_temp])\n",
    "# dictionary of start coordinates for each contig\n",
    "tig_starts=[0]+list(np.cumsum([len(j.seq) for j in fasta_temp]))[:-1]\n",
    "tig_names=[j.id for j in fasta_temp]\n",
    "fasta = pd.Series(tig_starts, index=tig_names).sort_values()\n",
    "tig_order = pd.Series(range(16), index=tig_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strains = [\"barcode11.ont\", 'barcode11_sub.ont', \"A2565.pb\", \"T58.pb\", 's288c.pb', 's288c_translocation.pb']\n",
    "# input the total bases mapped for each strain\n",
    "cov = dict(zip(strains,\n",
    "               np.array([709805531, 113027745, 105848406, 139709850, 674174505, 933961654])/12e6))\n",
    "# strains aliases \n",
    "strains_alias = dict(zip(strains, [\"Jean-Talon\",\"Jean-Talon\",\"A.2565\",\"A.T-58\",\"S288C\",\"S288C_translocation\"]))\n",
    "strains_alias = {s:strains_alias[s]+f'\\n{v:.0f}X' for s,v in cov.items()}\n",
    "strains_svim = strains[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the VCF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VCF={}\n",
    "for s in strains_svim:\n",
    "    with open(\"/Volumes/MacintoshHD/Dropbox/Jean_Talon/svim/\"+s+\"/final_results.vcf\", \"r\") as fi:\n",
    "        vcf = [line.split(\"\\t\") for line in fi.read().splitlines() if line[0]!=\"#\"]\n",
    "        VCF[s] = pd.DataFrame(vcf).astype({1:int, 5:int})\n",
    "        # filter the types of structural variants\n",
    "        VCF[s] = VCF[s].loc[\n",
    "            (VCF[s][4].isin(['<DEL>','<DUP:TANDEM>','<DUP_INT>','<INS>','<INV>']))\n",
    "        ]\n",
    "\n",
    "        for f in [\"pos_abs\",\"SVTYPE\", \"END\", \"SVLEN\", \"SUPPORT\", \"STD_SPAN\", \"STD_POS\"]:\n",
    "            VCF[s][f] = np.repeat(0, VCF[s].shape[0])\n",
    "        # get absolute coordinate for variants\n",
    "        for i in VCF[s].index:\n",
    "            chrom = VCF[s].loc[i, 0]\n",
    "            pos = VCF[s].loc[i, 1]\n",
    "            VCF[s].loc[i, \"pos_abs\"] = fasta[chrom]+int(pos)\n",
    "            \n",
    "            for j in VCF[s].loc[i, 7].split(\";\"):\n",
    "                j_split = j.split(\"=\")\n",
    "                if len(j_split)==2:\n",
    "                    VCF[s].loc[i, j_split[0]] = j_split[1]\n",
    "        # convert coordinates into rad for circular maps\n",
    "        VCF[s][\"pos_rad\"] = VCF[s][\"pos_abs\"]/len(FASTA)*2*np.pi\n",
    "        VCF[s][\"strain\"] = s\n",
    "        VCF[s] = VCF[s].astype({\"END\":int, \"SVLEN\":int, \"SUPPORT\":int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SV = {}\n",
    "for s in strains_svim:\n",
    "    threshold = np.ceil(0.15*cov[s])\n",
    "    # filter out the hits that have a score below 15% of the coverage depth\n",
    "    SV[s]=VCF[s].copy().drop(VCF[s].loc[VCF[s][5]<threshold].index, axis=0)\n",
    "SV = pd.concat(SV.values(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# circular map of the predicted structural variants\n",
    "color_sv = {\"DEL\":\"red\", \"INS\":\"green\", \"DUP:TANDEM\":\"deepskyblue\", \"DUP_INT\":\"navy\", \"INV\":\"fuchsia\"}\n",
    "alias_sv = {\"DEL\":\"Deletions\", \"INS\":\"Insertions\", \"DUP:TANDEM\":\"Tandem\\nduplications\", \"DUP_INT\":\"Interspersed\\nduplications\", \"INV\":\"Inversions\"}\n",
    "shift_sv = {\"DEL\":0, \"INS\":0.12, \"DUP:TANDEM\":0.24, \"DUP_INT\":0.36, \"INV\":0.48}\n",
    "strains_svim = strains[:5]\n",
    "y_pos_dict = dict(zip(strains_svim, range(5,10)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[10,10], subplot_kw={\"projection\":\"polar\"},\n",
    "                      gridspec_kw={'left':0.02, 'bottom':0.02, 'right':0.98, 'top':0.98})\n",
    "\n",
    "ax.set_theta_zero_location(\"N\")\n",
    "ax.set_theta_direction(\"clockwise\")\n",
    "ax.grid(False)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "xshrink = 0.9 # leave space at rad=0 for strain IDs\n",
    "yscale = 10\n",
    "\n",
    "tig_bound = sorted(list(fasta.values)+[len(FASTA)])\n",
    "for i in range(len(tig_bound)):\n",
    "    if (i%2 == 0) and (i!=16):\n",
    "        pos1 = tig_bound[i]/len(FASTA)*2*np.pi*xshrink\n",
    "        pos2 = tig_bound[i+1]/len(FASTA)*2*np.pi*xshrink\n",
    "        for j in np.arange(pos1, pos2, 0.01):\n",
    "            ax.plot([j, j], [4.5,9.5], color=\"0.9\", linewidth=5)\n",
    "    # plot the chromosome IDs\n",
    "    if i!=16:\n",
    "        pos = np.mean([tig_bound[i], tig_bound[i+1]])/len(FASTA)*2*np.pi*xshrink\n",
    "        ax.text(pos, 3.8, fasta.index[i], va=\"center\", ha=\"center\", size=10)\n",
    "        \n",
    "            \n",
    "# plot histograms of counts of SVs in 10kb windows\n",
    "for (s,t), df in SV.groupby(['strain','SVTYPE']):\n",
    "    y_pos = y_pos_dict[s]\n",
    "    ax.text(-0.03, y_pos, strains_alias[s], ha=\"right\", va=\"center\", size=8)\n",
    "\n",
    "    color = color_sv[t]\n",
    "    hs = np.histogram(df['pos_rad']*xshrink, np.arange(0, 2*np.pi*xshrink, 1e4*2*np.pi*xshrink/len(FASTA)))\n",
    "    ax.plot((hs[1][:-1]+hs[1][1:])/2, y_pos+hs[0]/yscale, color=color, lw=0.5)\n",
    "    ax.plot((hs[1][:-1]+hs[1][1:])/2, np.repeat(y_pos, hs[0].shape[0]), color='black', lw=0.5, zorder=10)\n",
    "\n",
    "# plot the y scale\n",
    "for i in range(0,11,2):\n",
    "    ax.plot(np.array([1,1.002])*2*np.pi*xshrink, np.repeat(9+i/yscale, 2), color='black', lw=0.5)\n",
    "    ax.text(1.002*2*np.pi*xshrink, 9+i/yscale, i, ha='left', va='bottom', rotation=(1-xshrink)*360, size=6)\n",
    "\n",
    "legend_elements=[Line2D([0], [0], \n",
    "                        #marker=\"s\", ms=10, color=\"#00000000\", mec=\"#00000000\", mfc=color_sv[i],\n",
    "                        color=color_sv[i],\n",
    "                        label=alias_sv[i]) for i in color_sv.keys()]\n",
    "ax.legend(handles=legend_elements, loc='center', frameon=False)\n",
    "\n",
    "plt.savefig(\"/Volumes/MacintoshHD/Dropbox/Jean_Talon/fig/Fig5A.svg\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the distributions of distance to the nearest same-type SV for each pair of strains\n",
    "strains_svim = strains[:4]\n",
    "\n",
    "distances = {}\n",
    "\n",
    "for (tig, sv), df in SV.groupby([0,'SVTYPE']):\n",
    "    if sv in [\"INS\",\"DEL\",\"DUP:TANDEM\"]:\n",
    "\n",
    "        for s1, s2 in itertools.combinations(strains_svim, 2):\n",
    "            \n",
    "            key = frozenset((s1, s2))\n",
    "            if key not in distances:\n",
    "                distances[key] = []\n",
    "            \n",
    "            sub1 = df.loc[df[\"strain\"]==s1, \"pos_abs\"]\n",
    "            sub2 = df.loc[df[\"strain\"]==s2, \"pos_abs\"]\n",
    "            \n",
    "            if all([sub1.shape[0]>0, sub2.shape[0]>0]):\n",
    "                # iterate over the s1 SVs and find the doistance to the closest same-type SV in s2\n",
    "                for i in sub1.index:\n",
    "                    value = min(abs(sub2-sub1.loc[i]))\n",
    "                    distances[key].append([value, sv, s1, s2])\n",
    "                # iterate over the s2 SVs and find the doistance to the closest same-type SV in s1\n",
    "                for i in sub2.index:\n",
    "                    value = min(abs(sub1-sub2.loc[i]))\n",
    "                    distances[key].append([value, sv, s1, s2])\n",
    "\n",
    "for key in distances:\n",
    "    new = pd.DataFrame(distances[key], columns=['dist','sv','s1','s2'])\n",
    "    new['dist_log'] = np.log10(new['dist'].replace(to_replace={0:1}).values)\n",
    "    distances[key] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distributions of distances and perform pairwise Mann-Whitney tests between distributions\n",
    "strains_svim = strains[:4]\n",
    "fig = plt.figure(figsize=[10,10])\n",
    "gs = plt.GridSpec(nrows=3, ncols=3, hspace=0.3, wspace=0.3, left=0.12, bottom=0.12)\n",
    "strain_plot_order = dict(zip(strains_svim, range(4)))\n",
    "\n",
    "for (i,j) in itertools.combinations(strains_svim, 2):\n",
    "    row_ax = strain_plot_order[j]-1\n",
    "    col_ax = strain_plot_order[i]\n",
    "    ax = fig.add_subplot(gs[row_ax, col_ax])\n",
    "    \n",
    "    data = distances[frozenset([i,j])]\n",
    "    \n",
    "    for sv, df in data.groupby('sv'):\n",
    "        hs = np.histogram(df['dist_log'].values, np.arange(0,6,0.1))\n",
    "        ax.plot(hs[1][:-1], hs[0], color=color_sv[sv])\n",
    "        \n",
    "        # plot median of the distribution as dotted line\n",
    "        ax.axvline(np.median(df['dist_log'].values), color=color_sv[sv], linestyle=':', lw=2)\n",
    "        \n",
    "        ax.set_xlim(-0.2,6.2)\n",
    "        ax.set_xticks(range(7))\n",
    "        ax.set_ylim(0,250)\n",
    "        \n",
    "        \n",
    "        if col_ax==0:\n",
    "            ax.set_ylabel(strains_alias[j])\n",
    "        else:\n",
    "            ax.set_yticklabels([])\n",
    "            \n",
    "        if row_ax==2:\n",
    "            ax.set_xlabel(strains_alias[i])\n",
    "        else:\n",
    "            ax.set_xticklabels([])\n",
    "    if (row_ax, col_ax) == (0,0):\n",
    "        ax.legend(handles=legend_elements[:3], loc=1, frameon=False)\n",
    "\n",
    "MWU = {}\n",
    "# reference distribution for distances is between Jean-Talon and Jean-Talon subset\n",
    "ref_distro = distances[frozenset(['barcode11.ont','barcode11_sub.ont'])]\n",
    "for sv, (row_ax, col_ax) in zip([\"INS\",\"DEL\",\"DUP:TANDEM\"], [(0,1),(0,2),(1,2)]):\n",
    "    \n",
    "    ax = fig.add_subplot(gs[row_ax, col_ax])\n",
    "    \n",
    "    ref_distro_sv = ref_distro.loc[ref_distro['sv']==sv, 'dist'].values\n",
    "    mwu=[]\n",
    "    \n",
    "    for (i,j) in itertools.combinations(strains_svim, 2):\n",
    "        distro1 = distances[frozenset([i,j])]\n",
    "        distro1_sv = distro1.loc[distro1['sv']==sv, 'dist'].values\n",
    "        s, p = mannwhitneyu(ref_distro_sv, distro1_sv, alternative='two-sided')\n",
    "        \n",
    "        mwu.append([s,p,i,j])\n",
    "    mwu = pd.DataFrame(mwu, columns=['U','p','col','row'])\n",
    "    # correct p-values for multiple testing\n",
    "    mwu['pval_corr'] = np.log10(sm.stats.multipletests(mwu['p'].values, method='fdr_bh', alpha=0.05)[1])*(-1)\n",
    "    #mwu['U_corr'] = mwu['U']/max(mwu['U'])\n",
    "    # normalize U statistics by the maximum possible value\n",
    "    mwu['U_norm'] = mwu['U']/(distro1_sv.shape[0]*ref_distro_sv.shape[0])\n",
    "    MWU[sv] = mwu\n",
    "    mwu_U = pd.pivot_table(mwu, index='row', columns='col', values='U_norm', \n",
    "                         aggfunc=lambda x: x).loc[strains_svim[1:], strains_svim[:-1]]\n",
    "    mwu_p = pd.pivot_table(mwu, index='row', columns='col', values='pval_corr', \n",
    "                     aggfunc=lambda x: x).loc[strains_svim[1:], strains_svim[:-1]]\n",
    "    \n",
    "    sns.heatmap(mwu_U, ax=ax, cmap='viridis', cbar_kws={'format':'%.2f'}\n",
    "                #, vmin=0, vmax=1\n",
    "               )\n",
    "    \n",
    "    for (i,j) in itertools.combinations(strains_svim, 2):\n",
    "        if mwu_p.loc[j,i] >= np.log10(0.05)*(-1):\n",
    "            ax.scatter(strain_plot_order[i]+0.5, strain_plot_order[j]-0.5, color='red', marker='o', s=30)\n",
    "    \n",
    "    ax.text(2, 0.5, alias_sv[sv], ha='center', va='center')\n",
    "    ax.set_xticklabels([strains_alias[s] for s in strains_svim[:-1]], size=6.5, rotation=0)\n",
    "    ax.set_yticklabels([strains_alias[s] for s in strains_svim[1:]], size=6.5, rotation=90, va='center')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "        \n",
    "\n",
    "fig.text(0.5, 0.015, '$log_{10}$ distance to closest variant (bp)', size=16, ha='center', va='center')\n",
    "fig.text(0.015, 0.5, 'Frequency', rotation=90, size=16, ha='center', va='center')\n",
    "\n",
    "sns.despine()\n",
    "plt.savefig('/Volumes/MacintoshHD/Dropbox/Jean_Talon/fig/Fig5b.svg')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of translocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate alternate reference for S288c with 2 translocations\n",
    "ASSEMBLY={}\n",
    "with open('/Users/mathieu/mhenault_landrylab/Sequences/ref_genomes/S288C_pacbio/S288c.genome.fa') as file_in:\n",
    "    for seq in SeqIO.parse(file_in, \"fasta\"):\n",
    "        ASSEMBLY[seq.id] = seq\n",
    "with open('/Users/mathieu/mhenault_landrylab/Sequences/ref_genomes/S288C_pacbio/S288c.all_feature.gff') as fi:\n",
    "    gff = pd.DataFrame([i.split('\\t') for i in fi.read().splitlines() if i[0]!='#']).astype({3:int,4:int})\n",
    "\n",
    "gff['tig_len'] = [len(ASSEMBLY[i].seq) for i in gff[0].values]\n",
    "gff['diff'] = gff['tig_len']-gff[4]\n",
    "TY1 = gff.loc[(gff[2]=='TY1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a good subtelomeric TY1 is index 10528; chrVIII, pos 562-568k, minus strand.\n",
    "# could be paired with index 16799; TY1 located half way on chrXIII\n",
    "# Another translocation: 12275-4930 chrXIV-chrIV\n",
    "translocations = [[10528,16799], [18938,4930], [12217,6196]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the S288c assembly with translocations\n",
    "NEW_ASSEMBLY = ASSEMBLY.copy()\n",
    "for t in translocations:\n",
    "    gff1_tig, gff2_tig = gff.loc[t, 0]\n",
    "    gff1_pos, gff2_pos = gff.loc[t, 4]\n",
    "\n",
    "    new_seq_1 = ASSEMBLY[gff1_tig].seq[:gff1_pos] + ASSEMBLY[gff2_tig].seq[gff2_pos:]\n",
    "    new_seq_2 = ASSEMBLY[gff2_tig].seq[:gff2_pos] + ASSEMBLY[gff1_tig].seq[gff1_pos:]\n",
    "    new_chrom_1 = SeqRecord.SeqRecord(seq=new_seq_1, id='_'.join([gff1_tig,gff2_tig]), description='')\n",
    "    new_chrom_2 = SeqRecord.SeqRecord(seq=new_seq_2, id='_'.join([gff2_tig,gff1_tig]), description='')\n",
    "    NEW_ASSEMBLY[new_chrom_1.id] = new_chrom_1\n",
    "    NEW_ASSEMBLY[new_chrom_2.id] = new_chrom_2\n",
    "    \n",
    "    del NEW_ASSEMBLY[gff1_tig]\n",
    "    del NEW_ASSEMBLY[gff2_tig]\n",
    "    \n",
    "#with open('/Volumes/MacintoshHD/Dropbox/Jean_Talon/data/S288c_genome_translocations.fasta','w') as fo:\n",
    "#    SeqIO.write(NEW_ASSEMBLY.values(), fo, 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aln_len_cigar(cigar):\n",
    "    pattern = '[0-9]+[MIDNSHP=X]'\n",
    "    aln_chunks = re.findall(pattern, cigar)\n",
    "    seq = pd.Series([i[:-1] for i in aln_chunks], index=[i[-1] for i in aln_chunks]).astype(int)\n",
    "    return(seq.loc[[i for i in ['M','I','S','=','X'] if i in seq.index]].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse sam files of reads with supplementary alignments\n",
    "def sam_table(sam_path):\n",
    "    SAM=[]\n",
    "    with open(sam_path, 'r') as fi:\n",
    "        for line in fi.read().splitlines():\n",
    "            line_split = line.split('\\t')\n",
    "            read = line_split[0]\n",
    "            flag = line_split[1]\n",
    "            aln_len = aln_len_cigar(line_split[5])\n",
    "            tig = line_split[2]\n",
    "            aln_start = int(line_split[3])\n",
    "            SAM.append([read, flag, tig, aln_start, aln_len])\n",
    "\n",
    "    SAM = pd.DataFrame(SAM, columns=['read','flag','tig','aln_start','aln_len'])\n",
    "\n",
    "    SAM['abs_start'] = SAM['aln_start']+fasta.loc[SAM['tig']].values\n",
    "    SAM['abs_mid'] = SAM['abs_start']+0.5*SAM['aln_len']\n",
    "    SAM['tig_order'] = tig_order.loc[SAM['tig']].values\n",
    "    SAM = SAM.sort_values(by='abs_start')\n",
    "\n",
    "    # keep only reads for which supplementary alignments are found on exactly two different chromosomes\n",
    "    for read, df in SAM.groupby('read'):\n",
    "        if len(set(df['tig']))==2:\n",
    "            #\n",
    "            df=df.sort_values(by='aln_len', ascending=False).iloc[:2].sort_values(by='tig_order')\n",
    "            SAM.loc[df.index, 'read_no'] = (0,1)\n",
    "\n",
    "    SAM = SAM.loc[np.invert(np.isnan(SAM['read_no']))]\n",
    "    SAM_pivot = pd.pivot_table(SAM, values='abs_mid', index='read', columns='read_no')\n",
    "    \n",
    "    return(SAM, SAM_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM = {}\n",
    "SAM_pivot = {}\n",
    "\n",
    "for s in strains:\n",
    "    sam_path = f'/Volumes/MacintoshHD/Dropbox/Jean_Talon/translocations/{s}.supp.sam'\n",
    "    sam, sam_pivot = sam_table(sam_path)\n",
    "    SAM[s] = sam\n",
    "    SAM_pivot[s] = sam_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export SAM\n",
    "#with open('/Volumes/MacintoshHD/Dropbox/Jean_Talon/translocations/SAM.pkl','wb') as fo:\n",
    "#    pkl.dump(SAM, fo)\n",
    "#with open('/Volumes/MacintoshHD/Dropbox/Jean_Talon/translocations/SAM_pivot.pkl','wb') as fo:\n",
    "#    pkl.dump(SAM_pivot, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import SAM\n",
    "with open('/Volumes/MacintoshHD/Dropbox/Jean_Talon/translocations/SAM.pkl','rb') as fi:\n",
    "    SAM = pkl.load(fi)\n",
    "with open('/Volumes/MacintoshHD/Dropbox/Jean_Talon/translocations/SAM_pivot.pkl','rb') as fi:\n",
    "    SAM_pivot = pkl.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot matrices of supplementary read mappings \n",
    "fig = plt.figure(figsize=[30,45])\n",
    "gs = plt.GridSpec(ncols=18, nrows=3, left=0.1, bottom=0.1, right=0.98, top=0.98)\n",
    "# define bin width\n",
    "wdw=2e4\n",
    "\n",
    "bins = pd.interval_range(start=1, end=(np.ceil(len(FASTA)/wdw)+1)*wdw, freq=wdw, closed='left')\n",
    "ticks = fasta.copy()\n",
    "ticks.loc['chr0'] = len(FASTA)\n",
    "ticks_pos = (ticks/wdw).apply(lambda x: np.ceil(x))\n",
    "labelpos = (ticks_pos[:-1].values+ticks_pos[1:].values)/2\n",
    "\n",
    "for s, ax_idx in zip(strains, itertools.product(range(3), range(2))):\n",
    "\n",
    "    c = cov[s]\n",
    "    ax = fig.add_subplot(gs[ax_idx[0], ax_idx[1]*9:(ax_idx[1]+1)*9])\n",
    "\n",
    "    matrix = pd.DataFrame(np.zeros([len(bins), len(bins)]), index=bins, columns=bins)\n",
    "\n",
    "    SAM_bins = pd.concat([pd.cut(SAM_pivot[s][0], bins), pd.cut(SAM_pivot[s][1], bins)], axis=1)\n",
    "    for (i,j), df in SAM_bins.groupby([0,1]):\n",
    "        reads = df.index\n",
    "        # approximate supplementary mapping depth by summing alignment lengths\n",
    "        coverage_sum = SAM[s].loc[SAM[s]['read'].isin(reads), 'aln_len'].sum()\n",
    "        matrix.loc[j,i] = coverage_sum/(wdw*c)\n",
    "    matrix = matrix.values\n",
    "    matrix[np.triu_indices_from(matrix)] = np.nan\n",
    "\n",
    "    HM = ax.imshow(matrix, cmap='viridis', vmin=0, vmax=2)\n",
    "\n",
    "    for i in ticks_pos:\n",
    "        ax.plot((i,i), (0, len(bins)-1), color='white', lw=0.1)\n",
    "        ax.plot((0, len(bins)-1), (i,i),  color='white', lw=0.1)\n",
    "\n",
    "    for i, tig in zip(labelpos, tig_names):\n",
    "        ax.text(i, i, s=tig, ha='left', va='bottom')\n",
    "\n",
    "    # add circles around simulated translocations\n",
    "    if s=='s288c_translocation.pb':\n",
    "        for t in translocations:\n",
    "            gff_sub = gff.loc[t].copy()\n",
    "            gff_sub['pos_abs'] = gff_sub.apply(lambda x: fasta[x[0]]+np.mean(x[[3,4]]), axis=1)\n",
    "            gff_sub = np.round(gff_sub['pos_abs'].sort_values()/2e4).astype(int)-1\n",
    "            ax.scatter(gff_sub.iloc[0], gff_sub.iloc[1], marker='o', s=100, c=(0,0,0,0), edgecolors='red', linewidths=0.5)\n",
    "\n",
    "    ax.set_xticks(ticks_pos)\n",
    "    ax.set_xticklabels([f'{i:.3f}' for i in ticks/1e6], rotation=-90)\n",
    "    ax.set_xlabel('Read segment 1 position (Mb)')\n",
    "\n",
    "    ax.set_yticks(ticks_pos)\n",
    "    ax.set_yticklabels([f'{i:.3f}' for i in ticks/1e6])\n",
    "    ax.set_ylabel('Read segment 2 position (Mb)')\n",
    "\n",
    "    ax.margins(0)\n",
    "\n",
    "    ax.set_title(strains_alias[s], size=20)\n",
    "\n",
    "    #plot the colorbar\n",
    "    ax = fig.add_subplot(gs[ax_idx[0], ax_idx[1]*9+7])\n",
    "    ax.axis('off')\n",
    "    fig.colorbar(HM, ax=ax, shrink=2, orientation='vertical', drawedges=False, label='Approx. Normalized read depth')\n",
    "\n",
    "sns.despine(trim=True)\n",
    "\n",
    "plt.savefig('/Volumes/MacintoshHD/Dropbox/Jean_Talon/fig/FigS6.svg')\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the read length distributions for long-read datasets\n",
    "#import read lenghts\n",
    "rl={}\n",
    "for s in strains[:6]:\n",
    "    rl[s]=[]\n",
    "    S='.'.join(s.split('.')[:-1])\n",
    "    with open(f'/Volumes/MacintoshHD/Dropbox/Jean_Talon/data/{S}.size.fasta') as fi:\n",
    "        for seq in SeqIO.parse(fi, 'fasta'):\n",
    "            rl[s].append(len(seq.seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('/Volumes/MacintoshHD/Dropbox/Jean_Talon/Files_SX_SVs/rl.pkl', 'wb') as fo:\n",
    "#    pkl.dump(rl, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[6,6])\n",
    "wdw=200\n",
    "for s in rl.keys():\n",
    "    h = np.histogram(rl[s], bins=np.arange(8e3,20e3+wdw,wdw), density=True)\n",
    "    ax.plot(h[1][:-1]+wdw/2, h[0], label=strains_alias[s], lw=2)\n",
    "    \n",
    "plt.legend(frameon=False)\n",
    "ax.set_xlabel('Read size (bp)')\n",
    "ax.set_ylabel('Density')\n",
    "sns.despine(trim=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/Volumes/MacintoshHD/Dropbox/Jean_Talon/fig/FigS5.svg')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually split the Jean-Talon assembly\n",
    "JT={}\n",
    "tig_order = []\n",
    "with open('/Volumes/MacintoshHD/Dropbox/Jean_Talon/assemblies/Jean-Talon_reordered.fasta') as fi:\n",
    "    for seq in SeqIO.parse(fi, 'fasta'):\n",
    "        JT[seq.id] = seq\n",
    "        tig_order.append(seq.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty2 = [93000+47862-1, 93000+53823]\n",
    "\n",
    "JT_split = []\n",
    "for tig in tig_order:\n",
    "    if tig=='ctg6_pilon':\n",
    "        new = SeqRecord.SeqRecord(JT['ctg6_pilon'].seq[:ty2[1]], id='ctg6.1_pilon')\n",
    "        JT_split.append(new)\n",
    "        new = SeqRecord.SeqRecord(JT['ctg6_pilon'].seq[ty2[0]:], id='ctg6.2_pilon')\n",
    "        JT_split.append(new)\n",
    "    else:\n",
    "        JT_split.append(JT[tig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/MacintoshHD/Dropbox/Jean_Talon/assemblies/Jean-Talon_reordered_split.fasta', 'w') as fo:\n",
    "    SeqIO.write(JT_split, fo, 'fasta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
